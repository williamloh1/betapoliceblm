if
or
because
as
until
while
of
at
by
for
with
about
against
between
into
through
during
before
after
above
below
to
from
up
down
in
out
on
off
over
under
again
further
then
once
here
there
when
where
why
how
all
any
both
each
few
more
most
other
some
such
no
nor
not
only
own
same
so
than
too
very
s
t
can
will
just
don
should
now")
####  Remove the standard english stop words
stop.words <- c("i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now")
text.as.data.4 <- str_replace_all(text.as.data.3, pattern = stop.words, replacement = "")
text.as.data.4
####  Remove the standard english stop words
stop.words <- ["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
####  Remove the standard english stop words
stop.words <- c("i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now")
text.as.data.4 <- str_replace_all(text.as.data.3, pattern = [stop.words], replacement = "")
####  Remove the standard english stop words
stop.words <- c("i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now")
text.as.data.4 <- str_replace_all(text.as.data.3, pattern = stop.words, replacement = "")
text.as.data.4
####  Remove the standard english stop words
text.as.data.4 <- str_replace_all(text.as.data.3, pattern = c("i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"), replacement = "")
text.as.data.4
####  Remove the standard english stop words
text.as.data.4 <- str_replace_all(text.as.data.3, pattern = c("i"), replacement = "")
text.as.data.4
####  Remove the standard english stop words
text.as.data.4 <- str_replace_all(text.as.data.3, pattern = c("i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"), replacement = "")
text.as.data.4
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
####  Remove the standard english stop words
stopwords_regex = paste(stopwords('en'), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
text.as.data.4 <- str_replace_all(text.as.data.3, pattern = stopwords_regex, replacement = "")
text.as.data.4
####  Add new words:
new.words <- c("see", "people","new","want","one","even","must","need","done","back","just","going","know", "can","said","like","many","like","realdonaldtrump")
text.as.data.5 <- c(text.as.data.4, new.words)
as.data.frame(sapply(text.as.data.5, function(x) gsub("\"", "", x)))
as.data.frame(sapply(text.as.data.5, function(x) gsub("\\\"", "", x)))
as.data.frame(sapply(text.as.data.5, function(x) gsub("\\"", "", x)))
as.data.frame(sapply(text.as.data.5, function(x) gsub("\"", "", x)))
text.as.data.4 <- str_replace_all(text.as.data.3, pattern = "\\"\\"", replacement = "")
text.as.data.4 <- str_replace_all(text.as.data.3, pattern = "(\"\")\\1", replacement = "")
text.as.data.3 <- str_replace_all(text.as.data.2, pattern = "[\\$\\.\\!\\/\\:\\#\\-\"]", replacement = "")
text.as.data.3
text.as.data.3
####  Remove the standard english stop words:
stopwords_regex = paste(stopwords('en'), collapse = '\\b|\\b')
####  Remove the standard english stop words:
stopwords_regex = paste(stopwords('en'), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
text.as.data.4 <- str_replace_all(text.as.data.3, pattern = stopwords_regex, replacement = "")
####  Add new words:
new.words <- c("see", "people","new","want","one","even","must","need","done","back","just","going","know", "can","said","like","many","like","realdonaldtrump")
text.as.data.6 <- str_replace_all(text.as.data.5, pattern = "(\"\")\\1", replacement = "")
text.as.data.6
as.factor(text.as.data.6)
as.factor(text.as.data.6) == NA
as.factor(text.as.data.6) == NULL
as.factor(text.as.data.6)
levesl(as.factor(text.as.data.6))
levels(as.factor(text.as.data.6))
levels(as.factor(text.as.data.6)[ text.as.data.6=! ""])
levels(as.factor(text.as.data.6)[ text.as.data.6 !=  ""])
(as.factor(text.as.data.6)[ text.as.data.6 !=  ""])
((text.as.data.6)[ text.as.data.6 !=  ""])
text.as.data.6 <- ((text.as.data.6)[ text.as.data.6 !=  ""])
text.as.data.6 <- ((text.as.data.6)[ text.as.data.6 !=  ""])
text.as.data.6 <- str_replace_all(text.as.data.5, pattern = "(\"\")\\1", replacement = "")
text.as.data.7 <- ((text.as.data.6)[ text.as.data.6 !=  ""])
text.as.data.7
text.as.data.2 <- str_replace_all(text.as.data.1, pattern = "\\d", replacement = "")
text.as.data.3 <- str_replace_all(text.as.data.2, pattern = "[\\$\\.\\!\\/\\:\\#\\-\"\\&]", replacement = "")
####  Remove the standard english stop words:
stopwords_regex = paste(stopwords('en'), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
text.as.data.4 <- str_replace_all(text.as.data.3, pattern = stopwords_regex, replacement = "")
####  Add new words:
new.words <- c("see", "people","new","want","one","even","must","need","done","back","just","going","know", "can","said","like","many","like","realdonaldtrump")
text.as.data.5 <- c(text.as.data.4, new.words)
text.as.data.6 <- str_replace_all(text.as.data.5, pattern = "(\"\")\\1", replacement = "")
text.as.data.7 <- ((text.as.data.6)[ text.as.data.6 !=  ""])
text.as.data.7
text.as.data.3 <- str_replace_all(text.as.data.2, pattern = "[\\$\\.\\!\\/\\:\\#\\-\"\\&\\;]", replacement = "")
####  Remove the standard english stop words:
stopwords_regex = paste(stopwords('en'), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
text.as.data.4 <- str_replace_all(text.as.data.3, pattern = stopwords_regex, replacement = "")
####  Add new words:
new.words <- c("see", "people","new","want","one","even","must","need","done","back","just","going","know", "can","said","like","many","like","realdonaldtrump")
text.as.data.5 <- c(text.as.data.4, new.words)
text.as.data.6 <- str_replace_all(text.as.data.5, pattern = "(\"\")\\1", replacement = "")
text.as.data.7 <- ((text.as.data.6)[ text.as.data.6 !=  ""])
text.as.data.7
text.as.data <- ((text.as.data.6)[ text.as.data.6 !=  ""])
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
install.packages("SnowballC")
#install.packages("SnowballC")
library(SnowballC)
wordcloud(words = text.as.data, freq = d$freq, min.freq = 3,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
text.as.data <- as.data.frame((text.as.data.6)[ text.as.data.6 !=  ""])
text.as.data <- as_tibble((text.as.data.6)[ text.as.data.6 !=  ""])
text.as.data <- text.as.data %>%
mutate(count = n())
View(text.as.data)
text.as.data <- text.as.data %>%
group_by(value) %>%
mutate(count = n())
View(text.as.data)
wordcloud(words = text.as.data$value, text.as.data$freq = $count, min.freq = 3,
wordcloud(words = text.as.data$value, text.as.data$freq = count, min.freq = 3,
wordcloud(words = text.as.data$value, freq = text.as.data$count, min.freq = 3,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
####  I exclude his retweets as they do not reflect his words:
tweets.pop50words <- tweets.sep %>%
filter(is_retweet == FALSE) %>%
select(text)
tweets.pop50words <- unlist(tweets.pop50words)
tweets.pop50words
tweets.pop50words <- str_split(unlist(tweets.pop50words), pattern = " ")
View(tweets.pop50words)
View(tweets.pop50words)
tweets.pop50words <- unlist(str_split(unlist(tweets.pop50words), pattern = " "))
tweets.pop50words
tweets.pop50words <- tweets.pop50words %>%
group_by(value) %>%
mutate(count = n())
tweets.pop50words <- tweets.pop50words %>%
group_by(chracter) %>%
mutate(count = n())
View(tweets.pop.favorite)
tweets.pop50words <- as_tibble(unlist(str_split(unlist(tweets.pop50words), pattern = " ")))
tweets.pop50words <- as_tibble(unlist(str_split(unlist(tweets.pop50words), pattern = " ")))
tweets.pop50words <- tweets.pop50words %>%
group_by(value) %>%
mutate(count = n())
tweets.pop50words <- tweets.pop50words %>%
group_by(value) %>%
mutate(count = n()) %>%
arrange(desc(count)) %>%
filter()
tweets.pop50words <- tweets.pop50words %>%
group_by(value) %>%
mutate(count = n()) %>%
arrange(desc(count)) %>%
slice(1:50)
tweets.pop50words <- tweets.pop50words %>%
group_by(value) %>%
mutate(count = n()) %>%
arrange(desc(count)) %>%
slice(1:50)
View(tweets.pop50words)
####  I exclude his retweets as they do not reflect his words:
tweets.pop50words <- tweets.sep %>%
filter(is_retweet == FALSE) %>%
select(text)
tweets.pop50words <- tweets.pop50words %>%
group_by(value) %>%
mutate(count = n()) %>%
arrange(desc(count))
tweets.pop50words <- tweets.pop50words %>%
group_by(value) %>%
mutate(count = n()) %>%
arrange(desc(count))
tweets.pop50words <- tweets.pop50words %>%
group_by(text) %>%
mutate(count = n()) %>%
arrange(desc(count))
####  I exclude his retweets as they do not reflect his words:
tweets.pop50words <- tweets.sep %>%
filter(is_retweet == FALSE) %>%
select(text)
tweets.pop50words.ready <- as_tibble(unlist(str_split(unlist(tweets.pop50words), pattern = " ")))
tweets.pop50words <- tweets.pop50words %>%
group_by(text) %>%
mutate(count = n()) %>%
arrange(desc(count))
tweets.pop50words <- tweets.pop50words %>%
group_by(text) %>%
mutate(count = n())
View(tweets.pop50words)
####  I exclude his retweets as they do not reflect his words:
tweets.pop50words <- tweets.sep %>%
filter(is_retweet == FALSE) %>%
select(text)
tweets.pop50words.ready <- as_tibble(unlist(str_split(unlist(tweets.pop50words), pattern = " ")))
tweets.pop50words <- tweets.pop50words %>%
group_by(text) %>%
mutate(count = n())
rm(list = ls())
library(tidyverse)
#install.packages('tm')
library(tm)
#install.packages('lubridate')
library(lubridate)
#install.packages('wordcloud')
library(wordcloud)
#install.packages("SnowballC")
library(SnowballC)
tweets <- read_csv('https://politicaldatascience.com/PDS/Datasets/trump_tweets.csv')
tweets.sep <- separate(data = tweets, col = created_at, into = c("date", "time"), sep = " ")
tweets.sep$date<-as.Date(tweets.sep$date, "%m/%d/%Y")
summary(tweets.sep$date)
#### As it can be seen here the coverage is ("2014-01-01" to "2020-02-14").
####  Let's check the most Retweeeted ones:
tweets.pop.retweet <- tweets.sep %>%
filter(is_retweet == FALSE) %>%
arrange(desc(retweet_count)) %>%
select(text) %>%
slice(1:5)
tweets.pop.retweet
####  Let's check the most favorited ones:
tweets.pop.favorite <- tweets.sep %>%
filter(is_retweet == FALSE) %>%
arrange(desc(favorite_count)) %>%
select(text) %>%
slice(1:5)
tweets.pop.favorite
####  Remove extraneous whitespace
text.as.data <- unlist(str_split(c(unlist(tweets.pop.favorite), unlist(tweets.pop.retweet)), " "))
####  Convert everything to lower case:
text.as.data.1 <- str_to_lower(text.as.data)
####  Remove numbers and punctuation:
text.as.data.2 <- str_replace_all(text.as.data.1, pattern = "\\d", replacement = "")
text.as.data.3 <- str_replace_all(text.as.data.2, pattern = "[\\$\\.\\!\\/\\:\\#\\-\"\\&\\;]", replacement = "")
####  Remove the standard english stop words:
stopwords_regex = paste(stopwords('en'), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
text.as.data.4 <- str_replace_all(text.as.data.3, pattern = stopwords_regex, replacement = "")
####  Add new words:
new.words <- c("see", "people","new","want","one","even","must","need","done","back","just","going","know", "can","said","like","many","like","realdonaldtrump")
text.as.data.5 <- c(text.as.data.4, new.words)
text.as.data.6 <- str_replace_all(text.as.data.5, pattern = "(\"\")\\1", replacement = "")
text.as.data <- as_tibble((text.as.data.6)[ text.as.data.6 !=  ""])
####  I exclude his retweets as they do not reflect his words:
tweets.pop50words <- tweets.sep %>%
filter(is_retweet == FALSE) %>%
select(text)
tweets.pop50words.ready <- as_tibble(unlist(str_split(unlist(tweets.pop50words), pattern = " ")))
tweets.pop50words <- tweets.pop50words %>%
group_by(text) %>%
mutate(count = n())
tweets.pop50words.ready <- as_tibble(unlist(str_split(unlist(tweets.pop50words), pattern = " ")))
tweets.pop50words.ready <- tweets.pop50words.ready %>%
group_by(text) %>%
mutate(count = n())
####  I exclude his retweets as they do not reflect his words:
tweets.pop50words <- tweets.sep %>%
filter(is_retweet == FALSE) %>%
select(text)
tweets.pop50words.ready <- as_tibble(unlist(str_split(unlist(tweets.pop50words), pattern = " ")))
tweets.pop50words.ready <- tweets.pop50words.ready %>%
group_by(text) %>%
mutate(count = n())
tweets.pop50words.ready <- tweets.pop50words.ready %>%
group_by(value) %>%
mutate(count = n())
tweets.pop50words.ready <- tweets.pop50words.ready %>%
group_by(value) %>%
mutate(count = n()) $>$
tweets.pop50words.ready <- tweets.pop50words.ready %>%
group_by(value) %>%
mutate(count = n()) %>%
filter(unique(value))
tweets.pop50words.ready <- tweets.pop50words.ready %>%
group_by(value) %>%
mutate(count = n()) %>%
filter(unique(value)[1])
tweets.pop50words.ready <- tweets.pop50words.ready %>%
group_by(value) %>%
mutate(count = n()) %>%
sapply(., function(x) unbag(unique(x)))
####  I exclude his retweets as they do not reflect his words:
tweets.pop50words <- tweets.sep %>%
filter(is_retweet == FALSE) %>%
str_to_lower(text) %>%
select(text)
####  I exclude his retweets as they do not reflect his words:
tweets.pop50words <- tweets.sep %>%
filter(is_retweet == FALSE) %>%
mutate(text = str_to_lower(text)) %>%
select(text)
tweets.pop50words.ready <- as_tibble(unlist(str_split(unlist(tweets.pop50words), pattern = " ")))
tweets.pop50words.ready <- tweets.pop50words.ready %>%
group_by(value) %>%
mutate(count = n())
paste(unique(trimws(unlist(tweets.pop50words.ready))),collapse) =
" ")
paste(unique(trimws(unlist(tweets.pop50words.ready))),collapse = " ")
tweets.pop50words.ready.2 <- tweets.pop50words.ready %>%
distinct()
View(tweets.pop50words.ready.2)
View(tweets.pop50words.ready.2)
tweets.pop50words.ready.2 <- tweets.pop50words.ready %>%
distinct() $>$
tweets.pop50words.ready.2 <- tweets.pop50words.ready %>%
distinct() %>%
arrange(desc(value)) %>%
slice(1:50)
tweets.pop50words.ready.2 <- tweets.pop50words.ready %>%
distinct() %>%
arrange(desc(count)) %>%
slice(1:50)
View(tweets.pop50words.ready.2)
View(tweets.pop50words.ready)
View(tweets.pop50words.ready.2)
tweets.pop50words.ready.2 <- tweets.pop50words.ready %>%
distinct() %>%
arrange(desc(count)) %>%
slice(1:50)
?slice
tweets.pop50words.ready.2 <- tweets.pop50words.ready %>%
distinct() %>%
arrange(desc(count)) %>%
slice(tweets.pop50words.ready.2, 1:50)
tweets.pop50words.ready.2 <- tweets.pop50words.ready %>%
distinct() %>%
arrange(desc(count))
View(tweets.pop50words.ready.2)
tweets.pop50words.ready.2 <- tweets.pop50words.ready %>%
distinct() %>%
arrange(desc(count)) %>%
filter(count%in%c(1:50))
tweets.pop50words.ready.2 <- tweets.pop50words.ready %>%
distinct() %>%
arrange(desc(count)) %>%
filter(%in%c(1:50))
tweets.pop50words.ready.2 <- tweets.pop50words.ready %>%
distinct() %>%
arrange(desc(count)) %>%
filter(tweets.pop50words.ready.2%in%c(1:50))
tweets.pop50words.ready.2 <- tweets.pop50words.ready %>%
distinct() %>%
arrange(desc(count))
View(tweets.pop50words.ready.2)
tweets.pop50words.ready.2 <- tweets.pop50words.ready.2[tweets.pop50words.ready.2$value != NULL]
tweets.pop50words.ready.2 <- tweets.pop50words.ready.2[,tweets.pop50words.ready.2$value != NULL]
tweets.pop50words.ready.2 <- tweets.pop50words.ready.2[, tweets.pop50words.ready.2$value != ""]
tweets.pop50words.ready.2 <- tweets.pop50words.ready.2[tweets.pop50words.ready.2$value != ""]
tweets.pop50words.ready.2 <- tweets.pop50words.ready %>%
distinct() %>%
arrange(desc(count)) %>%
select(value) %>%
slice(1:51)
View(tweets.pop50words.ready.2)
View(tweets.pop50words.ready.2)
tweets.pop50words.ready.2 <- tweets.pop50words.ready %>%
distinct() %>%
arrange(desc(count)) %>%
select(value)
tweets.pop50words.ready.2 <- tweets.pop50words.ready.2[1:50,]
View(tweets.pop50words.ready.2)
tweets.pop50words.ready.2 <- tweets.pop50words.ready %>%
distinct() %>%
arrange(desc(count))
tweets.pop50words.ready.2 <- tweets.pop50words.ready.2[1:50,]
tweets.pop50words.ready.2 <- tweets.pop50words.ready.2[tweets.pop50words.ready.2$value != ""]
tweets.pop50words.ready.2 <- tweets.pop50words.ready.2[, tweets.pop50words.ready.2$value != ""]
tweets.pop50words.ready.2 <- tweets.pop50words.ready.2[value != ""]
wordcloud(words = tweets.pop50words.ready.2$value, freq = tweets.pop50words.ready.2$count, min.freq = 3,
max.words=, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wc <- wordcloud(words = tweets.pop50words.ready.2$value, freq = tweets.pop50words.ready.2$count, min.freq = 3,
max.words=, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
ggsave("word_cloud.pdf")
wc <- wordcloud(words = tweets.pop50words.ready.2$value, freq = tweets.pop50words.ready.2$count, min.freq = 3,
max.words=, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
tweets.pop50words.ready.2 <- tweets.pop50words.ready.2[1:50,]
wc <- wordcloud(words = tweets.pop50words.ready.2$value, freq = tweets.pop50words.ready.2$count, min.freq = 3,
max.words=, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
install.packages("webshot")
library(webshot)
webshot("tmp.html","my_wordcloud.png", delay =5, vwidth = 480, vheight=480) # changed to png.
library("htmlwidgets")
saveWidget(my_graph,"tmp.html",selfcontained = F)
saveWidget(wc,"tmp.html",selfcontained = F)
wc <- wordcloud2(words = tweets.pop50words.ready.2$value, freq = tweets.pop50words.ready.2$count, min.freq = 3,
max.words=, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wc <- wordcloud(words = tweets.pop50words.ready.2$value, freq = tweets.pop50words.ready.2$count, min.freq = 3,
max.words=, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
####  I exclude his retweets as they do not reflect his words:
####  Also because the question did not specifically asked, I would not exclude the stop words.
tweets.pop50words <- tweets.sep %>%
filter(is_retweet == FALSE) %>%
mutate(text = str_to_lower(text)) %>%
select(text)
####  Getting number of each tweet
tweets.pop50words.ready <- as_tibble(unlist(str_split(unlist(tweets.pop50words), pattern = " ")))
tweets.pop50words.ready <- tweets.pop50words.ready %>%
group_by(value) %>%
mutate(count = n() )
####  Getting rid of dublicates
tweets.pop50words.ready.2 <- tweets.pop50words.ready %>%
distinct() %>%
arrange(desc(count))
####  Getting first 50 words
tweets.pop50words.ready.2 <- tweets.pop50words.ready.2[1:50,]
wc <- wordcloud(words = tweets.pop50words.ready.2$value, freq = tweets.pop50words.ready.2$count, min.freq = 3,
max.words=, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
\
####  I exclude his retweets as they do not reflect his words:
####  Also because the question did not specifically asked, I would not exclude the stop words.
tweets.pop50words <- tweets.sep %>%
filter(is_retweet == FALSE) %>%
mutate(text = str_to_lower(text)) %>%
select(text)
####  Getting number of each tweet
tweets.pop50words.ready <- as_tibble(unlist(str_split(unlist(tweets.pop50words), pattern = " ")))
tweets.pop50words.ready <- tweets.pop50words.ready %>%
group_by(value) %>%
mutate(count = n() )
####  Getting rid of dublicates
tweets.pop50words.ready.2 <- tweets.pop50words.ready %>%
distinct() %>%
arrange(desc(count))
####  Getting first 50 words
tweets.pop50words.ready.2 <- tweets.pop50words.ready.2[1:50,]
wc <- wordcloud(words = tweets.pop50words.ready.2$value, freq = tweets.pop50words.ready.2$count, min.freq = 3,
max.words=, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
####  Getting number of each tweet
tweets.pop50words.ready <- tweets.pop50words.ready %>%
group_by(value) %>%
mutate(count = n() )
DTM <- TermDocumentMatrix(tweets.pop50words.ready., control = list(weighting = weightTfIdf))
DTM <- TermDocumentMatrix(tweets.pop50words.ready.2, control = list(weighting = weightTfIdf))
DTM <- TermDocumentMatrix(as.vector(tweets.pop50words.ready.2$value), control = list(weighting = weightTfIdf))
DTM <- TermDocumentMatrix(as.character(tweets.pop50words.ready.2$value), control = list(weighting = weightTfIdf))
DTM <- TermDocumentMatrix(as.character(tweets.pop50words.ready.2$value), control = list(weighting = weightTfIdf))
DTM <- TermDocumentMatrix(tweets.sep, control = list(weighting = weightTfIdf))
DTM <- TermDocumentMatrix(tweets.sep$text, control = list(weighting = weightTfIdf))
DTM <- TermDocumentMatrix(as.list(tweets.sep$text), control = list(weighting = weightTfIdf))
?TermDocumentMatrix
DTM <- TermDocumentMatrix(as.matrixtweets.sep$text), control = list(weighting = weightTfIdf))
DTM <- TermDocumentMatrix(as.matrixtweets.sep$text) control = list(weighting = weightTfIdf))
DTM <- TermDocumentMatrix(as.matrix(tweets.sep$text), control = list(weighting = weightTfIdf))
DTM <- TermDocumentMatrix((tweets.sep$text), control = list(weighting = weightTfIdf))
